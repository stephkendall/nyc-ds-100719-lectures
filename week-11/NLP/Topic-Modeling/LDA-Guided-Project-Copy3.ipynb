{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling Assessment Project\n",
    "\n",
    "Welcome to your Topic Modeling Assessment! For this project you will be working with a dataset of over 400,000 quora questions that have no labeled cateogry, and attempting to find optimal number of cateogries to assign these questions to. The .csv file of these text questions can be found in the NLP folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Import pandas and read in the quora_questions.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('quora_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Task: Create a vectorized document term matrix. \n",
    "\n",
    "- How do you want to clean up your text with regards to stopwords, special characters, and other situations.\n",
    "- Using a Countvectorizer versus a TFIDFvectorizer\n",
    "- You may want to explore the max_df and min_df parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    # remove html tags from all of the text before processing\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    # we disabled the parser and ner parts of the pipeline in order to speed up parsing\n",
    "    mytokens = nlp(cleantext, disable=['parser', 'ner'])\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer, max_df=0.90, min_df=100, stop_words='english', use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.fit_transform(df1['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x34 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6711 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dtm.todok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-61181116104b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mCV_rfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mCV_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV_rfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rfc' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, y)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(d)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniekendall/anaconda3/lib/python3.7/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el25341124242490483720624220\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el25341124242490483720624220_data = {\"mdsDat\": {\"x\": [-10.878067970275879, -60.88402557373047, -2.3361191749572754, -61.385318756103516, -28.737743377685547, 1.0666056871414185, 45.14072036743164, -140.7959747314453, 61.531681060791016, -72.56305694580078, 15.984491348266602, -160.85162353515625, 85.96183013916016, -99.02083587646484, -176.76412963867188, -117.18988037109375, 108.82991790771484, 47.588096618652344, -106.46783447265625, -50.61380386352539], \"y\": [10.538727760314941, -75.5918197631836, -53.2890739440918, -12.914633750915527, 126.2969970703125, -123.55281066894531, -7.947869300842285, -98.53054809570312, -83.24056243896484, -140.39688110351562, 63.57818603515625, 68.94514465332031, 51.922935485839844, 109.93634796142578, -8.681683540344238, -37.42020034790039, -22.946203231811523, 121.934814453125, 30.176889419555664, 59.32884216308594], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [8.728007844697942, 7.08169185697705, 7.031819565186267, 5.82302929164981, 5.81766858328537, 5.28559421752037, 5.181608458967157, 5.119132870317462, 4.933186128246353, 4.931643140863378, 4.888888397997289, 4.82934389225168, 4.581145092758514, 4.542535316426417, 4.539151642630141, 4.353700803690255, 3.396691218226134, 3.36800258306392, 3.2534868866369755, 2.3136722086074992]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"Freq\": [621.0, 278.0, 272.0, 236.0, 191.0, 189.0, 218.0, 182.0, 183.0, 159.0, 159.0, 183.0, 157.0, 146.0, 138.0, 146.0, 146.0, 138.0, 149.0, 132.0, 155.0, 128.0, 124.0, 123.0, 121.0, 114.0, 111.0, 111.0, 96.0, 99.0, 485.29973041932817, 0.034776563410240664, 0.03477656330464201, 0.03477656328566437, 0.03477656318734381, 0.03477656322838759, 0.03477656348101179, 0.03477656327717042, 0.03477656338978777, 0.03477658762398094, 0.03477656346857786, 0.03477656349944556, 0.03477656357956013, 0.03477656341355545, 0.03477656340475595, 0.03477656326511562, 0.034776563369587184, 0.03477657667197807, 0.03477656356362382, 0.03477656341202988, 0.03477656351528713, 0.0347765634103455, 0.03477656351303382, 0.0347765632921099, 0.03477656344558925, 0.03477658538052827, 0.03477656348432958, 0.03477656347740854, 0.03477656338649277, 0.03477656331561835, 0.034776563704128204, 0.03477656349267255, 0.034776563475552615, 0.03477656334829113, 178.7378776489433, 91.13146104663853, 74.64712370553579, 48.93876846364274, 0.04120812292520733, 0.04120812301143002, 0.04120812300073433, 0.041208123038941245, 0.04120812314094477, 0.04120812308763847, 0.04120812303886454, 0.04120812310450763, 0.041208133297792895, 0.04120812283898744, 0.04120812272246022, 0.041208122761541115, 0.041208122954696565, 0.04120812392884069, 0.0412081229510245, 0.041208122894056044, 0.041208122993657795, 0.04120812295309974, 0.04120825822094251, 0.041208122860388385, 0.041208122869880154, 0.04120812288323544, 0.04120812287958439, 0.04120812302827198, 0.04120812297490715, 0.041208122876776526, 0.041208123348736886, 0.04120812312446135, 0.04120812310787304, 150.53163995511255, 94.39088043732959, 90.32267631877481, 55.422883373098635, 0.04146029983304925, 0.04146029985097609, 0.04146030026158263, 0.041460300139514704, 0.041460299848350576, 0.04146030001009662, 0.04146029988588258, 0.0414603001957218, 0.041460300318214585, 0.0414602999540253, 0.04146031138113618, 0.04146029996533961, 0.04146029995797633, 0.04146029998862327, 0.04146030018243853, 0.04146033154197665, 0.04146030005032805, 0.041460300020450945, 0.04146030007460098, 0.041460300032501916, 0.04146030019797267, 0.04146029997842983, 0.0414603155244034, 0.04146030015319439, 0.041460300165136896, 0.041460300183428535, 0.04146030054717157, 0.0414603001868176, 0.0414603001570212, 0.04146030008811, 231.29543058038226, 91.85468550929512, 0.04346804031574151, 0.04346804018089558, 0.04346804019311594, 0.04346804050185973, 0.043468040243986275, 0.04346804016066282, 0.043468040296146065, 0.04346804050351244, 0.04346804006199725, 0.043468040027233326, 0.04346803999428723, 0.04346804015496658, 0.04346804036428139, 0.04346804036018838, 0.04346804030101403, 0.04346804034637056, 0.043468040245879275, 0.04346804036601844, 0.043468040390359085, 0.04346804027400991, 0.04346804027314884, 0.043468040212794365, 0.04346804030638838, 0.04346805670481918, 0.04346804022446288, 0.043468040382122604, 0.0434680401888511, 0.04346804017917767, 0.04346804049261882, 0.04346804026415531, 0.04346804008257984, 178.37239523545495, 144.5060689686527, 0.04262047062609971, 0.04262047096635938, 0.04262047084605447, 0.04262047087909807, 0.04262047091991169, 0.042620470791980024, 0.042620471002265146, 0.04262047088712059, 0.042620470963312306, 0.042620470833970046, 0.04262047104454765, 0.04262047089486628, 0.042620470885939725, 0.042620470792608986, 0.042620470947397356, 0.042620470944597824, 0.04262047077092838, 0.04262047088067127, 0.04262047084685111, 0.04262047083578005, 0.04262047079242033, 0.04262047079401778, 0.04262047095545007, 0.04262047082217239, 0.04262047094764765, 0.04262047077330074, 0.04262047085478003, 0.042620470731975016, 0.04262052399722958, 0.042620471375550986, 0.042620470928579485, 0.04262047091611097, 213.39923079320945, 79.6424873400366, 0.048310428618900586, 0.04831042872946332, 0.04831042865321101, 0.0483104285451165, 0.048310428119308704, 0.048310428692281734, 0.04831042854633901, 0.04831042849819204, 0.04831055257179137, 0.04831042843454064, 0.048310428640207195, 0.048310428569992236, 0.048310428587025424, 0.04831042845989846, 0.04831042844246362, 0.04831042867346044, 0.04831042848502871, 0.048310428692884884, 0.048310428648455785, 0.04831042818752557, 0.048310428491449016, 0.048310445013460095, 0.048310428714989175, 0.0483104283750176, 0.04831042847674812, 0.048310448341957045, 0.048310428654969394, 0.04831042831257289, 0.04831042891324545, 0.04831042847435869, 0.04831042845352484, 153.0568108821512, 106.57533827891865, 27.656946886678796, 0.048484094181201046, 0.04848409446261671, 0.04848409431726955, 0.048484094454563344, 0.048484094281085334, 0.048484094328795406, 0.04848409428654384, 0.04848409446671267, 0.04848409430954248, 0.04848409426733467, 0.048484094519249274, 0.048484094566921696, 0.0484840942641706, 0.0484840944742594, 0.04848409450277583, 0.048484094373605895, 0.048484094181629606, 0.04848409427343194, 0.048484094623036614, 0.04848409477403376, 0.048484094343706534, 0.048484094633118126, 0.0484840941019218, 0.048484094495508355, 0.048484094340029975, 0.048484094232800146, 0.04848409414188569, 0.04848409466962919, 0.04848409459921813, 0.048484094522124016, 0.04848409420842434, 142.05506014050866, 141.68787126913006, 0.04897352980173409, 0.04897352938572154, 0.04897352953511944, 0.04897352928572833, 0.048973529464333684, 0.04897352939167663, 0.048973529234877654, 0.04897352932163599, 0.04897352904739287, 0.04897352918558701, 0.04897352968102624, 0.04897352946423702, 0.04897352914449331, 0.04897352927168612, 0.04897352946107744, 0.04897352930186035, 0.04897352927866929, 0.04897352936451896, 0.048973529408860295, 0.04897352935493363, 0.04897354591954186, 0.048973529174542056, 0.04897352934344755, 0.04897358061334594, 0.04897352923682933, 0.04897352934461082, 0.048973529349688065, 0.04897355535513161, 0.048973537978771305, 0.048973529424022784, 0.04897352939864107, 0.04897352924195667, 273.2844844129688, 0.05036460424174087, 0.05036460365376547, 0.05036461973603791, 0.050364603619254066, 0.05036460351401843, 0.05036460372773263, 0.05036460401120732, 0.05036460368335821, 0.05036463517395643, 0.05036460349487096, 0.0503646036249542, 0.05036460369853466, 0.05036460376733396, 0.05036460358015931, 0.050364603546366016, 0.05036460357314258, 0.050364603499639414, 0.05036462987874062, 0.050364603710592815, 0.050364603828592217, 0.05036460347818343, 0.05036464974257288, 0.05036460363219054, 0.050364603747008185, 0.05036460350042115, 0.05036460378540903, 0.05036470564903612, 0.05036460378561061, 0.05036460367441099, 0.05036465187570529, 0.05036463021002191, 0.05036460364699776, 0.050364603627341735, 154.62653528612626, 118.61540786418176, 0.050580510312137034, 0.05058051052478518, 0.050580510708434306, 0.050580510605080876, 0.05058051019954868, 0.050580510643865005, 0.0505805106236979, 0.05058053010019884, 0.05058051033957782, 0.050580510498059984, 0.050580510549382715, 0.05058051713938083, 0.050580524553438, 0.050580510272135316, 0.050580510352047005, 0.05058051064473479, 0.05058051064819543, 0.05058051042048492, 0.05058053113034026, 0.05058051061303529, 0.0505805104574405, 0.05058051042990196, 0.05058051042505992, 0.05058051058312536, 0.050580524359039813, 0.050580510487927846, 0.0505805105310341, 0.05058051059499593, 0.05058062052577936, 0.050580510996542934, 0.05058051064787879, 0.05058051047094893, 154.3185142709213, 116.45712113307475, 0.05318714518114498, 0.05318714585778736, 0.05318714530041649, 0.05318714541533075, 0.05318714507628897, 0.05318714501150156, 0.053187145395502476, 0.053187145209407254, 0.05318714522306914, 0.05318714523771196, 0.05318714547988805, 0.053187145033474645, 0.053187145374851516, 0.053187145158035964, 0.053187145121444686, 0.05318714530067189, 0.05318714518646987, 0.05318714528689938, 0.05318714506555774, 0.05318714526715691, 0.05318714499514221, 0.05318714500063869, 0.05318714519809804, 0.05318714481647594, 0.05318714529753773, 0.05318714523654139, 0.053187145093257324, 0.0531871454729307, 0.05318714545406645, 0.05318714518868143, 0.053187145150499354, 0.05318714503850621, 267.495644014196, 0.05040371089635017, 0.050403710793500464, 0.050403748968159856, 0.050403728383676266, 0.05040371084983955, 0.050403710956078596, 0.0504037108083011, 0.05040371071758055, 0.05040371079351598, 0.050403724459758346, 0.050403710639400814, 0.05040372591092833, 0.050403711044652064, 0.0504037845338791, 0.05040371088705122, 0.05040371075923522, 0.05040371054090392, 0.050403710866405174, 0.05040371084158852, 0.050403710983903935, 0.05040371083731004, 0.05040371092816136, 0.050403724096493886, 0.05040371107026043, 0.05040371092361038, 0.050403710486835306, 0.05040372696312372, 0.05040371073155086, 0.05040371081407857, 0.05040374550358283, 0.050403711074350294, 0.0504037108294759, 0.0504037107664457, 133.48449659356163, 120.0742190727015, 0.05522259062707431, 0.05522258968129334, 0.05522258942050008, 0.05522258966410683, 0.055222589972804215, 0.05522258990464141, 0.055222589772022386, 0.05522258974891905, 0.05522258980498162, 0.05522258982824981, 0.05522258976369639, 0.05522259042199078, 0.055222589550831605, 0.05522259004568453, 0.05522259000915332, 0.055222589474774895, 0.055222589632313725, 0.05522265116309809, 0.05522258981394857, 0.055222589771126186, 0.055222590093530644, 0.055222589618298845, 0.05522258983297448, 0.055222589451383156, 0.05522261788065878, 0.055222589443274704, 0.055222589887863376, 0.05522258947731195, 0.05522261517047045, 0.055222590030997144, 0.05522258978483658, 0.05522258977627019, 127.6815407465468, 123.81081040450621, 0.0525502413812627, 0.05255024117669369, 0.05255024094003257, 0.05255024138169892, 0.0525502412588406, 0.05255024112171913, 0.052550241453223866, 0.052550241006979914, 0.05255024109288912, 0.05255024117106419, 0.052550241312576386, 0.052550242402830405, 0.05255024102865023, 0.052550241066616085, 0.05255024112457757, 0.0525502411053738, 0.05255024114403598, 0.05255024117063074, 0.052550240932557765, 0.05255024088244914, 0.05255024110492047, 0.05255024121747566, 0.052550241130353316, 0.05255024120019107, 0.05255024101524767, 0.05255024092253876, 0.05255024145699538, 0.05255024096331268, 0.052550241221339236, 0.052550241163163314, 0.05255024114911855, 141.7427742666898, 109.4702901630823, 0.055384641904959585, 0.0553846420654463, 0.05538464159228874, 0.0553846420389566, 0.05538464232966466, 0.05538464500279037, 0.055384641706236346, 0.055384641642890343, 0.0553846418102087, 0.055384641902940304, 0.05538464171927545, 0.055384641702842526, 0.05538464168238804, 0.0553846416459512, 0.055384641571710636, 0.05538464171884235, 0.05538464184413771, 0.05538464139790823, 0.05538464180207165, 0.05538464185881298, 0.055384642086582864, 0.05538464230611994, 0.05538464159331118, 0.05538464195885774, 0.05538464145213166, 0.055384642601434365, 0.055384641686933446, 0.0553846416513724, 0.05538464292546179, 0.05538464221926971, 0.055384642208755885, 0.055384641778890895, 134.1836412309038, 106.68444203938137, 0.05566751572303539, 0.055667515601654304, 0.055667515277052014, 0.055667517551966486, 0.05566751555845364, 0.055667515480087526, 0.05566751551484935, 0.05566751573302116, 0.05566751528647002, 0.05566751549991716, 0.0556675158374427, 0.05566751582527159, 0.05566753020985417, 0.05566751574593902, 0.05566751556213584, 0.0556675154461401, 0.05566751543039504, 0.0556675154004987, 0.05566751549530347, 0.05566751568996474, 0.055667515476871446, 0.05566751530164629, 0.05566751544082941, 0.055667536352760005, 0.05566751665321883, 0.055667515709920616, 0.05566753772295822, 0.05566751584353551, 0.05566753668670052, 0.055667515747736256, 0.05566751554421691, 0.05566751540943974, 186.53484691494353, 0.084138248671319, 0.08413824913333103, 0.08413824856207681, 0.08413824838887443, 0.08413824861901344, 0.08413824911437198, 0.08413824890405647, 0.08413824871731522, 0.08413824879286619, 0.08413824857328103, 0.08413824833227046, 0.08413828568505997, 0.08413824889628622, 0.08413824829040971, 0.08413824859962181, 0.08413824847552291, 0.08413824844649499, 0.08413824897542137, 0.08413827350622738, 0.08413830781321487, 0.08413828012886448, 0.08413824891854707, 0.08413824851758427, 0.08413824914173794, 0.0841382482386687, 0.0841382489514194, 0.08413824863326114, 0.08413824856747115, 0.0841382947386749, 0.0841382787009494, 0.0841382486875985, 0.08413824858656022, 184.94173920560203, 0.08396169344639355, 0.0839616932338637, 0.08396169372650344, 0.08396169361763556, 0.08396169346781572, 0.08396169348561447, 0.08396169347488307, 0.08396169345361426, 0.08396169362104566, 0.08396169398000058, 0.08396169349007589, 0.08396169408772766, 0.083961693595322, 0.08396169336295925, 0.08396169290166036, 0.08396169287753308, 0.08396169361617661, 0.08396169411662391, 0.08396169318409619, 0.08396169348368979, 0.08396169320499883, 0.08396169364358118, 0.08396169323708187, 0.08396169421438439, 0.0839616932754421, 0.08396169321413864, 0.08396169338264506, 0.08396169364868732, 0.08396169260728172, 0.08396169427560862, 0.0839616938710581, 0.08396169374620652, 178.26060644411285, 0.09301343167265799, 0.09301343162047783, 0.09301343164916658, 0.09301343215294458, 0.09301343171536167, 0.09301343213115164, 0.09301343184160618, 0.0930134314965536, 0.09301343201297217, 0.0930134310551949, 0.09301343492968794, 0.09301343152367597, 0.0930134318203198, 0.09301343159325064, 0.09301343225737956, 0.09301343215168417, 0.09301343199310796, 0.09301343185935732, 0.09301343183092677, 0.09301343155233005, 0.0930134321743115, 0.09301343199865729, 0.09301343165585295, 0.09301343142680037, 0.09301343134331698, 0.09301343236358331, 0.09301343126001205, 0.09301343156086968, 0.09301343173901672, 0.09301351547192911, 0.09301346791683288, 0.09301343208408754, 0.09301343174763957, 3.7926575884287055, 3.792657581456319, 3.7926575676466516, 3.7926575680404504, 3.7926575593661696, 3.7926575600259778, 3.792657562645272, 3.7926575662178093, 3.7926575727267813, 3.7926575483223064, 3.7926575600904253, 3.79265756379992, 3.7926575979141215, 3.792657573962573, 3.7926575939624083, 3.7926575770439723, 3.7926575532501126, 3.7926575689883006, 3.792657579203364, 3.792657587863052, 3.7926575618009277, 3.792657588995177, 3.792657564491017, 3.792657569002101, 3.7926575755797667, 3.7926575647488363, 3.7926575747602196, 3.7926575610022906, 3.7926575519906907, 3.7926575739169364, 3.792657589596567, 3.7926575793848394, 3.7926575603652752], \"Term\": [\"good\", \"india\", \"like\", \"use\", \"day\", \"job\", \"people\", \"want\", \"difference\", \"quora\", \"time\", \"way\", \"work\", \"year\", \"trump\", \"think\", \"know\", \"money\", \"life\", \"mean\", \"learn\", \"happen\", \"thing\", \"start\", \"question\", \"girl\", \"lose\", \"movie\", \"world\", \"new\", \"good\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"india\", \"use\", \"like\", \"people\", \"way\", \"indian\", \"note\", \"good\", \"need\", \"book\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"job\", \"day\", \"india\", \"use\", \"people\", \"learn\", \"new\", \"book\", \"good\", \"note\", \"need\", \"indian\", \"world\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"use\", \"people\", \"india\", \"like\", \"use\", \"world\", \"note\", \"need\", \"book\", \"indian\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"india\", \"like\", \"good\", \"difference\", \"life\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"way\", \"job\", \"day\", \"people\", \"good\", \"use\", \"like\", \"india\", \"people\", \"need\", \"note\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"india\", \"use\", \"like\", \"work\", \"movie\", \"good\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"use\", \"like\", \"people\", \"india\", \"think\", \"know\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"year\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"like\", \"use\", \"india\", \"good\", \"india\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"like\", \"good\", \"people\", \"use\", \"time\", \"start\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"good\", \"india\", \"like\", \"use\", \"quora\", \"question\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"use\", \"good\", \"india\", \"like\", \"like\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"good\", \"india\", \"people\", \"use\", \"money\", \"thing\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"happen\", \"mean\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"good\", \"india\", \"use\", \"like\", \"mean\", \"happen\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"use\", \"good\", \"india\", \"year\", \"girl\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"good\", \"india\", \"like\", \"use\", \"trump\", \"lose\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"like\", \"good\", \"india\", \"use\", \"day\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"people\", \"good\", \"like\", \"use\", \"job\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"day\", \"people\", \"good\", \"india\", \"like\", \"want\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"india\", \"use\", \"good\", \"like\", \"note\", \"need\", \"book\", \"indian\", \"world\", \"new\", \"movie\", \"lose\", \"girl\", \"question\", \"start\", \"thing\", \"happen\", \"mean\", \"money\", \"trump\", \"know\", \"year\", \"think\", \"life\", \"learn\", \"work\", \"quora\", \"time\", \"want\", \"difference\", \"way\", \"job\", \"day\", \"people\", \"use\", \"good\", \"india\"], \"Total\": [621.0, 278.0, 272.0, 236.0, 191.0, 189.0, 218.0, 182.0, 183.0, 159.0, 159.0, 183.0, 157.0, 146.0, 138.0, 146.0, 146.0, 138.0, 149.0, 132.0, 155.0, 128.0, 124.0, 123.0, 121.0, 114.0, 111.0, 111.0, 96.0, 99.0, 621.9788338052788, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 278.0605533382862, 236.07839604886502, 272.27167382249013, 218.17735389198788, 183.52310316423217, 95.91668639472192, 79.43234905480955, 621.9788338052788, 84.42061037591193, 95.10764952222026, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 189.68421095507168, 191.27714212222523, 278.0605533382862, 236.07839604886502, 218.17735389198788, 155.31661326577273, 99.17585358333082, 95.10764952222026, 621.9788338052788, 79.43234905480955, 84.42061037591193, 95.91668639472192, 96.63765091080202, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 236.07839604886502, 218.17735389198788, 278.0605533382862, 272.27167382249013, 236.07839604886502, 96.63765091080202, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 278.0605533382862, 272.27167382249013, 621.9788338052788, 183.1562082981947, 149.28988210902077, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 621.9788338052788, 236.07839604886502, 272.27167382249013, 278.0605533382862, 218.17735389198788, 84.42061037591193, 79.43234905480955, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 278.0605533382862, 236.07839604886502, 272.27167382249013, 157.83476030539282, 111.35328763042607, 621.9788338052788, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 236.07839604886502, 272.27167382249013, 218.17735389198788, 278.0605533382862, 146.83252009717893, 146.4653311888064, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.51382313430713, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 272.27167382249013, 236.07839604886502, 278.0605533382862, 621.9788338052788, 278.0605533382862, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 272.27167382249013, 621.9788338052788, 218.17735389198788, 236.07839604886502, 159.40238822848403, 123.39126080957557, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 621.9788338052788, 278.0605533382862, 272.27167382249013, 236.07839604886502, 159.09176061822131, 121.23036755528591, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 236.07839604886502, 621.9788338052788, 278.0605533382862, 272.27167382249013, 272.27167382249013, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 621.9788338052788, 278.0605533382862, 218.17735389198788, 236.07839604886502, 138.25570747944747, 124.84542998328158, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 128.58469366250668, 132.4554240626528, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 621.9788338052788, 278.0605533382862, 236.07839604886502, 272.27167382249013, 132.4554240626528, 128.58469366250668, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 236.07839604886502, 621.9788338052788, 278.0605533382862, 146.51382313430713, 114.24133903242482, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 621.9788338052788, 278.0605533382862, 272.27167382249013, 236.07839604886502, 138.95440717472212, 111.4552079912013, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 272.27167382249013, 621.9788338052788, 278.0605533382862, 236.07839604886502, 191.27714212222523, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 218.17735389198788, 621.9788338052788, 272.27167382249013, 236.07839604886502, 189.68421095507168, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 191.27714212222523, 218.17735389198788, 621.9788338052788, 278.0605533382862, 272.27167382249013, 182.99402650683732, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 278.0605533382862, 236.07839604886502, 621.9788338052788, 272.27167382249013, 79.43234905480955, 84.42061037591193, 95.10764952222026, 95.91668639472192, 96.63765091080202, 99.17585358333082, 111.35328763042607, 111.4552079912013, 114.24133903242482, 121.23036755528591, 123.39126080957557, 124.84542998328158, 128.58469366250668, 132.4554240626528, 138.25570747944747, 138.95440717472212, 146.4653311888064, 146.51382313430713, 146.83252009717893, 149.28988210902077, 155.31661326577273, 157.83476030539282, 159.09176061822131, 159.40238822848403, 182.99402650683732, 183.1562082981947, 183.52310316423217, 189.68421095507168, 191.27714212222523, 218.17735389198788, 236.07839604886502, 621.9788338052788, 278.0605533382862], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.1905, -5.2951, -5.356, -5.4752, -5.4837, -5.4911, -5.5171, -5.6329, -5.6338, -5.6585, -5.7179, -5.7355, -5.7473, -5.7768, -5.8064, -5.8493, -5.8543, -5.907, -5.9073, -5.9095, -5.9261, -5.9656, -5.9817, -5.9897, -5.9916, -6.1296, -6.1305, -6.1325, -6.1655, -6.1739, -6.548, -6.3843, -6.527, -6.3055, 2.6212, 2.5965, 2.5855, 0.1053, -4.9773, -5.0965, -5.1124, -5.1384, -5.2542, -5.2551, -5.2798, -5.3392, -5.3568, -5.3685, -5.3981, -5.4277, -5.4706, -5.4756, -5.5283, -5.5286, -5.5308, -5.5474, -5.5869, -5.603, -5.6109, -5.6129, -5.7509, -5.7518, -5.7868, -5.7952, -6.1693, -6.0056, -5.9268, 2.6234, 2.6053, 2.6031, 0.2368, -4.9032, -4.9641, -5.0918, -5.0993, -5.241, -5.2419, -5.2666, -5.326, -5.3437, -5.3554, -5.3849, -5.4145, -5.4574, -5.4624, -5.5151, -5.5154, -5.5176, -5.5342, -5.5898, -5.5978, -5.5997, -5.7377, -5.7386, -5.7406, -5.7737, -5.782, -5.9925, -5.9136, -6.1561, -6.1351, 2.8229, 2.7926, -4.6673, -4.7282, -4.8474, -4.8559, -4.8893, -5.0051, -5.006, -5.0307, -5.0901, -5.1077, -5.1195, -5.149, -5.1786, -5.2215, -5.2265, -5.2792, -5.2795, -5.2817, -5.2983, -5.3378, -5.3539, -5.3619, -5.3638, -5.5018, -5.5027, -5.5047, -5.5377, -5.5461, -5.9202, -5.8992, -6.7253, 2.8178, 2.8117, -4.6861, -4.747, -4.8662, -4.8746, -4.8821, -4.908, -5.0239, -5.0248, -5.0495, -5.1088, -5.1265, -5.1382, -5.1677, -5.1974, -5.2403, -5.2453, -5.2979, -5.2983, -5.3004, -5.3566, -5.3727, -5.3806, -5.3826, -5.5206, -5.5235, -5.5565, -5.5649, -5.6965, -6.7441, -5.7753, -5.918, -5.939, 2.918, 2.8819, -4.4648, -4.6449, -4.6534, -4.6609, -4.6868, -4.8026, -4.8035, -4.8282, -4.8876, -4.9053, -4.917, -4.9465, -4.9762, -5.019, -5.0241, -5.0767, -5.077, -5.0792, -5.0958, -5.1354, -5.1515, -5.1594, -5.1614, -5.2994, -5.3003, -5.3023, -5.3353, -5.3436, -5.7178, -5.5541, -5.6967, 2.9293, 2.9162, -0.153, -4.4414, -4.5023, -4.6215, -4.6299, -4.6374, -4.6634, -4.7801, -4.8048, -4.8642, -4.8818, -4.8935, -4.9231, -4.9527, -4.9956, -5.0006, -5.0533, -5.0536, -5.0558, -5.0724, -5.1119, -5.1359, -5.1379, -5.2759, -5.2768, -5.2788, -5.3118, -5.3202, -5.5306, -5.6733, -5.4518, -5.6943, 2.9391, 2.939, -4.4192, -4.4801, -4.5993, -4.6078, -4.6153, -4.6412, -4.757, -4.7579, -4.7826, -4.842, -4.8597, -4.8714, -4.9009, -4.9305, -4.9734, -4.9784, -5.0314, -5.0502, -5.0898, -5.1058, -5.1138, -5.1157, -5.2537, -5.2546, -5.2566, -5.2897, -5.298, -5.4296, -5.6511, -5.5085, -5.6721, -6.4772, 2.9919, -4.3542, -4.4151, -4.5343, -4.5428, -4.5502, -4.5762, -4.692, -4.6929, -4.7176, -4.777, -4.7946, -4.8064, -4.8359, -4.8655, -4.9084, -4.9134, -4.9661, -4.9664, -4.9686, -4.9852, -5.0247, -5.0408, -5.0488, -5.0507, -5.1887, -5.1896, -5.1916, -5.2246, -5.233, -5.5861, -6.4122, -5.3646, -5.4434, 2.9791, 2.97, -4.3496, -4.4105, -4.5297, -4.5382, -4.5457, -4.5716, -4.6874, -4.6883, -4.713, -4.7724, -4.8018, -4.8313, -4.8609, -4.9038, -4.9088, -4.9615, -4.9618, -4.964, -4.9806, -5.0202, -5.0362, -5.0442, -5.1841, -5.185, -5.187, -5.2201, -5.2284, -5.36, -6.4076, -5.6025, -5.5815, -5.4389, 2.9877, 2.978, -4.2906, -4.3515, -4.4707, -4.4792, -4.4867, -4.5126, -4.6284, -4.6294, -4.654, -4.7311, -4.7428, -4.7723, -4.802, -4.8448, -4.8499, -4.9025, -4.9029, -4.905, -4.9216, -4.9612, -4.9773, -4.9872, -5.1252, -5.1261, -5.1281, -5.1611, -5.1695, -5.301, -5.3799, -6.3486, -5.5436, -5.5225, 3.0128, -4.3321, -4.393, -4.5122, -4.5207, -4.5282, -4.5541, -4.6699, -4.6709, -4.6955, -4.7549, -4.7726, -4.7843, -4.8138, -4.8435, -4.8863, -4.8914, -4.944, -4.9444, -4.9465, -4.9631, -5.0027, -5.0188, -5.0267, -5.0287, -5.1667, -5.1676, -5.1696, -5.2026, -5.211, -6.3901, -5.5851, -5.3425, -5.4214, 3.0481, 3.0443, -4.1881, -4.249, -4.3682, -4.3766, -4.3841, -4.4101, -4.5259, -4.5268, -4.5515, -4.6109, -4.6285, -4.6697, -4.6994, -4.7473, -4.8, -4.8003, -4.8025, -4.8191, -4.8586, -4.8747, -4.8826, -4.8846, -5.0226, -5.0235, -5.0255, -5.0585, -5.0669, -5.1985, -6.2461, -5.441, -5.2773, -5.42, 3.055, 3.0539, -4.2292, -4.2901, -4.4093, -4.4178, -4.4253, -4.4512, -4.567, -4.5679, -4.5926, -4.652, -4.6697, -4.6814, -4.7834, -4.7884, -4.8411, -4.8414, -4.8436, -4.8602, -4.8998, -4.9158, -4.9238, -4.9257, -5.0638, -5.0646, -5.0666, -5.0997, -5.108, -5.2396, -5.3185, -6.2872, -5.4821, 3.0593, 3.0498, -4.1759, -4.2368, -4.356, -4.3645, -4.372, -4.3979, -4.5137, -4.5146, -4.5987, -4.6164, -4.6281, -4.6576, -4.6873, -4.7301, -4.7352, -4.7878, -4.7903, -4.8069, -4.8465, -4.8626, -4.8705, -4.8725, -5.0105, -5.0114, -5.0134, -5.0464, -5.0547, -5.1863, -6.2339, -5.4289, -5.4078, -5.2652, 3.0992, 3.0904, -4.1291, -4.19, -4.3092, -4.3177, -4.3252, -4.3511, -4.4669, -4.4925, -4.5519, -4.5696, -4.5813, -4.6108, -4.6405, -4.6833, -4.741, -4.7413, -4.7435, -4.7601, -4.7997, -4.8158, -4.8237, -4.8256, -4.9637, -4.9646, -4.9666, -4.9996, -5.0079, -5.1395, -5.361, -6.1871, -5.3821, -5.2184, 3.3573, -3.4678, -3.5287, -3.6479, -3.6564, -3.6639, -3.6898, -3.8056, -3.8065, -3.8312, -3.8906, -3.9083, -3.92, -3.9495, -3.9792, -4.022, -4.0271, -4.0797, -4.08, -4.0822, -4.0988, -4.1384, -4.1545, -4.1624, -4.1644, -4.3024, -4.3033, -4.3053, -4.3383, -4.4782, -5.5258, -4.6997, -4.5571, 3.3655, -3.4614, -3.5224, -3.6416, -3.65, -3.6575, -3.6834, -3.7993, -3.8002, -3.8249, -3.8842, -3.9019, -3.9136, -3.9431, -3.9728, -4.0156, -4.0207, -4.0733, -4.0737, -4.0758, -4.0924, -4.132, -4.1481, -4.156, -4.158, -4.296, -4.2969, -4.2989, -4.3403, -4.4719, -5.5195, -4.7144, -4.6933, 3.3992, -3.3245, -3.3854, -3.5046, -3.513, -3.5205, -3.5465, -3.6623, -3.6632, -3.6879, -3.7473, -3.7649, -3.7766, -3.8062, -3.8358, -3.8787, -3.8837, -3.9364, -3.9367, -3.9389, -3.9555, -3.995, -4.0111, -4.019, -4.021, -4.1599, -4.1619, -4.1949, -4.2033, -4.3349, -4.5774, -4.4137, -5.3825, -4.5564, 0.7245, 0.6636, 0.5444, 0.5359, 0.5284, 0.5025, 0.3867, 0.3858, 0.3611, 0.3017, 0.284, 0.2723, 0.2428, 0.2132, 0.1703, 0.1653, 0.1126, 0.1123, 0.1101, 0.0935, 0.0539, 0.0379, 0.0299, 0.028, -0.1101, -0.1109, -0.1129, -0.146, -0.1543, -0.2859, -0.3648, -1.3335, -0.5284], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -0.0024, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -9.5459, -0.7922, -1.4658, -1.6653, -2.0875, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -9.1672, -0.9569, -1.4236, -1.4676, -1.956, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -9.1541, -0.3387, -1.2622, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -8.9181, -0.5976, -0.8082, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -8.9369, -0.3224, -1.308, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -8.7157, -0.6349, -0.9969, -2.3458, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -8.6922, -0.6974, -0.7, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -8.6701, -0.0061, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -8.605, -0.5753, -0.8404, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -8.6005, -0.5685, -0.85, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -8.5415, -0.0062, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -8.583, -0.6486, -0.7544, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -8.4389, -0.6845, -0.7153, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -8.4801, -0.5793, -0.8377, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -8.4268, -0.5924, -0.8217, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -8.38, -0.0148, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -7.7187, -0.0149, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -7.7123, -0.0171, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -7.5753, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264, -3.5264]}, \"token.table\": {\"Topic\": [3, 20, 17, 20, 5, 20, 15, 20, 1, 2, 3, 7, 20, 14, 20, 9, 20, 2, 20, 18, 20, 8, 20, 3, 20, 5, 20, 12, 20, 16, 20, 14, 20, 13, 20, 7, 20, 6, 20, 3, 20, 2, 20, 6, 20, 11, 20, 11, 20, 10, 20, 13, 20, 8, 20, 10, 20, 16, 20, 4, 20, 19, 20, 2, 20, 7, 20, 4, 20, 15, 20], \"Freq\": [0.9462961228893902, 0.04205760546175068, 0.9776390316439789, 0.02091206484799955, 0.9718480288159278, 0.02183928154642534, 0.9541204692030336, 0.035013595200111325, 0.779769300239303, 0.07878081590046566, 0.08842744641889004, 0.04501760908598038, 0.006431087012282912, 0.9643449501498209, 0.031107901617736158, 0.9818005348923781, 0.014385355822598946, 0.9487400307545187, 0.04170285849470412, 0.9753052142216456, 0.02108768030749504, 0.9695127088945696, 0.02731021715195971, 0.97220765264572, 0.02575384510319788, 0.9712647498382507, 0.026793510340365537, 0.9806381848376668, 0.014691208761612985, 0.9600269195894999, 0.035888856807084105, 0.9663628417319826, 0.030198838804124457, 0.9619856020755688, 0.028931897806784027, 0.9609056209918622, 0.03592170545763971, 0.94763588706327, 0.047381794353163496, 0.9478113533050472, 0.04033239801298073, 0.9441996981387122, 0.050357317234064655, 0.9762699757805706, 0.01833370846536283, 0.9568559622414685, 0.03299503318074029, 0.9679948188489773, 0.025142722567505903, 0.96441189772465, 0.03241720664620672, 0.9611885674635392, 0.032039618915451304, 0.9670882166022854, 0.027241921594430573, 0.9723819180038021, 0.025093726916227153, 0.9643450878928048, 0.02878642053411358, 0.9784885185012276, 0.016943524129891387, 0.9727093468449872, 0.02185863700775252, 0.9753540394302046, 0.02179562099285373, 0.9693682158731188, 0.025342959892107685, 0.9520098960695698, 0.041391734611720424, 0.9691918275167157, 0.02730117823990749], \"Term\": [\"book\", \"book\", \"day\", \"day\", \"difference\", \"difference\", \"girl\", \"girl\", \"good\", \"good\", \"good\", \"good\", \"good\", \"happen\", \"happen\", \"india\", \"india\", \"indian\", \"indian\", \"job\", \"job\", \"know\", \"know\", \"learn\", \"learn\", \"life\", \"life\", \"like\", \"like\", \"lose\", \"lose\", \"mean\", \"mean\", \"money\", \"money\", \"movie\", \"movie\", \"need\", \"need\", \"new\", \"new\", \"note\", \"note\", \"people\", \"people\", \"question\", \"question\", \"quora\", \"quora\", \"start\", \"start\", \"thing\", \"thing\", \"think\", \"think\", \"time\", \"time\", \"trump\", \"trump\", \"use\", \"use\", \"want\", \"want\", \"way\", \"way\", \"work\", \"work\", \"world\", \"world\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [11, 15, 6, 12, 18, 14, 3, 17, 2, 4, 19, 10, 7, 13, 5, 1, 8, 9, 20, 16]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el25341124242490483720624220\", ldavis_el25341124242490483720624220_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el25341124242490483720624220\", ldavis_el25341124242490483720624220_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el25341124242490483720624220\", ldavis_el25341124242490483720624220_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=                x           y  topics  cluster      Freq\n",
       "topic                                                   \n",
       "10     -10.878068   10.538728       1        1  8.728008\n",
       "14     -60.884026  -75.591820       2        1  7.081692\n",
       "5       -2.336119  -53.289074       3        1  7.031820\n",
       "11     -61.385319  -12.914634       4        1  5.823029\n",
       "17     -28.737743  126.296997       5        1  5.817669\n",
       "13       1.066606 -123.552811       6        1  5.285594\n",
       "2       45.140720   -7.947869       7        1  5.181608\n",
       "16    -140.795975  -98.530548       8        1  5.119133\n",
       "1       61.531681  -83.240562       9        1  4.933186\n",
       "3      -72.563057 -140.396881      10        1  4.931643\n",
       "18      15.984491   63.578186      11        1  4.888888\n",
       "9     -160.851624   68.945145      12        1  4.829344\n",
       "6       85.961830   51.922935      13        1  4.581145\n",
       "12     -99.020836  109.936348      14        1  4.542535\n",
       "4     -176.764130   -8.681684      15        1  4.539152\n",
       "0     -117.189880  -37.420200      16        1  4.353701\n",
       "7      108.829918  -22.946203      17        1  3.396691\n",
       "8       47.588097  121.934814      18        1  3.368003\n",
       "19    -106.467834   30.176889      19        1  3.253487\n",
       "15     -50.613804   59.328842      20        1  2.313672, topic_info=   Category        Freq        Term       Total  loglift  logprob\n",
       "4   Default  621.000000        good  621.000000  30.0000  30.0000\n",
       "6   Default  278.000000       india  278.000000  29.0000  29.0000\n",
       "12  Default  272.000000        like  272.000000  28.0000  28.0000\n",
       "28  Default  236.000000         use  236.000000  27.0000  27.0000\n",
       "1   Default  191.000000         day  191.000000  26.0000  26.0000\n",
       "8   Default  189.000000         job  189.000000  25.0000  25.0000\n",
       "20  Default  218.000000      people  218.000000  24.0000  24.0000\n",
       "29  Default  182.000000        want  182.000000  23.0000  23.0000\n",
       "2   Default  183.000000  difference  183.000000  22.0000  22.0000\n",
       "22  Default  159.000000       quora  159.000000  21.0000  21.0000\n",
       "26  Default  159.000000        time  159.000000  20.0000  20.0000\n",
       "30  Default  183.000000         way  183.000000  19.0000  19.0000\n",
       "31  Default  157.000000        work  157.000000  18.0000  18.0000\n",
       "33  Default  146.000000        year  146.000000  17.0000  17.0000\n",
       "27  Default  138.000000       trump  138.000000  16.0000  16.0000\n",
       "25  Default  146.000000       think  146.000000  15.0000  15.0000\n",
       "9   Default  146.000000        know  146.000000  14.0000  14.0000\n",
       "15  Default  138.000000       money  138.000000  13.0000  13.0000\n",
       "11  Default  149.000000        life  149.000000  12.0000  12.0000\n",
       "14  Default  132.000000        mean  132.000000  11.0000  11.0000\n",
       "10  Default  155.000000       learn  155.000000  10.0000  10.0000\n",
       "5   Default  128.000000      happen  128.000000   9.0000   9.0000\n",
       "24  Default  124.000000       thing  124.000000   8.0000   8.0000\n",
       "23  Default  123.000000       start  123.000000   7.0000   7.0000\n",
       "21  Default  121.000000    question  121.000000   6.0000   6.0000\n",
       "3   Default  114.000000        girl  114.000000   5.0000   5.0000\n",
       "13  Default  111.000000        lose  111.000000   4.0000   4.0000\n",
       "16  Default  111.000000       movie  111.000000   3.0000   3.0000\n",
       "32  Default   96.000000       world   96.000000   2.0000   2.0000\n",
       "18  Default   99.000000         new   99.000000   1.0000   1.0000\n",
       "..      ...         ...         ...         ...      ...      ...\n",
       "7   Topic20    3.792658      indian   95.916686   0.5359  -3.5264\n",
       "32  Topic20    3.792658       world   96.637651   0.5284  -3.5264\n",
       "18  Topic20    3.792658         new   99.175854   0.5025  -3.5264\n",
       "16  Topic20    3.792658       movie  111.353288   0.3867  -3.5264\n",
       "13  Topic20    3.792658        lose  111.455208   0.3858  -3.5264\n",
       "3   Topic20    3.792658        girl  114.241339   0.3611  -3.5264\n",
       "21  Topic20    3.792658    question  121.230368   0.3017  -3.5264\n",
       "23  Topic20    3.792658       start  123.391261   0.2840  -3.5264\n",
       "24  Topic20    3.792658       thing  124.845430   0.2723  -3.5264\n",
       "5   Topic20    3.792658      happen  128.584694   0.2428  -3.5264\n",
       "14  Topic20    3.792658        mean  132.455424   0.2132  -3.5264\n",
       "15  Topic20    3.792658       money  138.255707   0.1703  -3.5264\n",
       "27  Topic20    3.792658       trump  138.954407   0.1653  -3.5264\n",
       "9   Topic20    3.792658        know  146.465331   0.1126  -3.5264\n",
       "33  Topic20    3.792658        year  146.513823   0.1123  -3.5264\n",
       "25  Topic20    3.792658       think  146.832520   0.1101  -3.5264\n",
       "11  Topic20    3.792658        life  149.289882   0.0935  -3.5264\n",
       "10  Topic20    3.792658       learn  155.316613   0.0539  -3.5264\n",
       "31  Topic20    3.792658        work  157.834760   0.0379  -3.5264\n",
       "22  Topic20    3.792658       quora  159.091761   0.0299  -3.5264\n",
       "26  Topic20    3.792658        time  159.402388   0.0280  -3.5264\n",
       "29  Topic20    3.792658        want  182.994027  -0.1101  -3.5264\n",
       "2   Topic20    3.792658  difference  183.156208  -0.1109  -3.5264\n",
       "30  Topic20    3.792658         way  183.523103  -0.1129  -3.5264\n",
       "8   Topic20    3.792658         job  189.684211  -0.1460  -3.5264\n",
       "1   Topic20    3.792658         day  191.277142  -0.1543  -3.5264\n",
       "20  Topic20    3.792658      people  218.177354  -0.2859  -3.5264\n",
       "28  Topic20    3.792658         use  236.078396  -0.3648  -3.5264\n",
       "4   Topic20    3.792658        good  621.978834  -1.3335  -3.5264\n",
       "6   Topic20    3.792658       india  278.060553  -0.5284  -3.5264\n",
       "\n",
       "[703 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "0         3  0.946296        book\n",
       "0        20  0.042058        book\n",
       "1        17  0.977639         day\n",
       "1        20  0.020912         day\n",
       "2         5  0.971848  difference\n",
       "2        20  0.021839  difference\n",
       "3        15  0.954120        girl\n",
       "3        20  0.035014        girl\n",
       "4         1  0.779769        good\n",
       "4         2  0.078781        good\n",
       "4         3  0.088427        good\n",
       "4         7  0.045018        good\n",
       "4        20  0.006431        good\n",
       "5        14  0.964345      happen\n",
       "5        20  0.031108      happen\n",
       "6         9  0.981801       india\n",
       "6        20  0.014385       india\n",
       "7         2  0.948740      indian\n",
       "7        20  0.041703      indian\n",
       "8        18  0.975305         job\n",
       "8        20  0.021088         job\n",
       "9         8  0.969513        know\n",
       "9        20  0.027310        know\n",
       "10        3  0.972208       learn\n",
       "10       20  0.025754       learn\n",
       "11        5  0.971265        life\n",
       "11       20  0.026794        life\n",
       "12       12  0.980638        like\n",
       "12       20  0.014691        like\n",
       "13       16  0.960027        lose\n",
       "...     ...       ...         ...\n",
       "19        2  0.944200        note\n",
       "19       20  0.050357        note\n",
       "20        6  0.976270      people\n",
       "20       20  0.018334      people\n",
       "21       11  0.956856    question\n",
       "21       20  0.032995    question\n",
       "22       11  0.967995       quora\n",
       "22       20  0.025143       quora\n",
       "23       10  0.964412       start\n",
       "23       20  0.032417       start\n",
       "24       13  0.961189       thing\n",
       "24       20  0.032040       thing\n",
       "25        8  0.967088       think\n",
       "25       20  0.027242       think\n",
       "26       10  0.972382        time\n",
       "26       20  0.025094        time\n",
       "27       16  0.964345       trump\n",
       "27       20  0.028786       trump\n",
       "28        4  0.978489         use\n",
       "28       20  0.016944         use\n",
       "29       19  0.972709        want\n",
       "29       20  0.021859        want\n",
       "30        2  0.975354         way\n",
       "30       20  0.021796         way\n",
       "31        7  0.969368        work\n",
       "31       20  0.025343        work\n",
       "32        4  0.952010       world\n",
       "32       20  0.041392       world\n",
       "33       15  0.969192        year\n",
       "33       20  0.027301        year\n",
       "\n",
       "[71 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[11, 15, 6, 12, 18, 14, 3, 17, 2, 4, 19, 10, 7, 13, 5, 1, 8, 9, 20, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, dtm, vectorizer=tfidf, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search_params = {'n_components': [15, 20, 25],'learning_decay': [.5, .7]}\n",
    "lda = LatentDirichletAllocation(max_iter=25)\n",
    "model = GridSearchCV(lda, param_grid=search_params, cv=3, verbose=2, n_jobs = -1)\n",
    "model.fit(dtm)\n",
    "best_lda_model = model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare() takes from 5 to 11 positional arguments but 404289 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-80f6934b871c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: prepare() takes from 5 to 11 positional arguments but 404289 were given"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "dtm1 = pyLDAvis.prepare(*d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(dtm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Modelling\n",
    "\n",
    "#### TASK: Using Scikit-Learn create an instance of LDA. \n",
    "\n",
    "- You can manually run and tune your model, then evaluate the resulting clusters. \n",
    "- Or you can use gridsearch to try and identify the best number of topics to use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=200, doc_topic_prior=None,\n",
      "                          evaluate_every=-1, learning_decay=0.7,\n",
      "                          learning_method='online', learning_offset=10.0,\n",
      "                          max_doc_update_iter=100, max_iter=20,\n",
      "                          mean_change_tol=0.001, n_components=20, n_jobs=-1,\n",
      "                          perp_tol=0.1, random_state=100, topic_word_prior=None,\n",
      "                          total_samples=1000000.0, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=20,           # Number of topics\n",
    "                                      max_iter=20,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=200,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "\n",
    "print(lda_model)  # Model attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "\n",
    "lda_output = lda_model.fit_transform(dtm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Evaluate the different models you have run and determine which model you think determines the best clusters.  \n",
    "\n",
    "\n",
    "The evaluation part could invlove:\n",
    "- Printing out the top 15 most common words for each of the topics and seeing if they make sense.\n",
    "- Using the perplexity and log-likelihoood scores.\n",
    "- Using the pyLDAvis tool to investigate the different clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -104189.82761837322\n",
      "Perplexity:  246.96629388197618\n",
      "{'batch_size': 200, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 20, 'mean_change_tol': 0.001, 'n_components': 20, 'n_jobs': -1, 'perp_tol': 0.1, 'random_state': 100, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "# Log Likelihood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(dtm))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(dtm))\n",
    "\n",
    "# See model parameters\n",
    "print(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-81a8175f9ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'THE TOP 15 WORDS FOR TOPIC #{index}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(lda_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define Search Param\n",
    "search_params = {'n_components': [15, 20, 25],'learning_decay': [.5, .7]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation(max_iter=25)\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, cv=3, verbose=2, n_jobs = -1)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(dtm)\n",
    "# Create Document - Topic Matrix\n",
    "lda_output = lda_model.transform(dtm)\n",
    "\n",
    "# column names\n",
    "model = GridSearchCV(lda, param_grid=search_params, cv=3, verbose=2, n_jobs = -1)\n",
    "# best_lda_model = model.best_estimator_\n",
    "topicnames = [\"Topic\" + str(i) for i in range(model.n_topics)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK: Add a new column to the original quora dataframe that labels each question into one of the topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use  instead of ?</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Topic\n",
       "0  What is the step by step guide to invest in sh...      5\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...     16\n",
       "2  How can I increase the speed of my internet co...     17\n",
       "3  Why am I mentally very lonely? How can I solve...     11\n",
       "4  Which one dissolve in water quikly sugar, salt...     14\n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...      1\n",
       "6                                Should I buy tiago?      0\n",
       "7                     How can I be a good geologist?     10\n",
       "8                    When do you use  instead of ?     19\n",
       "9  Motorola (company): Can I hack my Charter Moto...     17"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
